{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDMicp5.ipynb",
      "provenance": [],
      "mount_file_id": "1mHkQNp9Hi1oU8o5tT2WLZQx_6kV1BHfA",
      "authorship_tag": "ABX9TyOIhrENu/dbs6Qds24psqaV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanamalaanusha/anusha-icp1-spring-2021/blob/main/KDMicp5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yGaEvUfim22",
        "outputId": "451ad3ca-1083-48b5-fb9c-c10f1c387562"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBoJLi4sjCH7"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7XOPCxwjGAG"
      },
      "source": [
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMyfaj_NjJ3T"
      },
      "source": [
        "# creating spark dataframe wiht the input data. You can also read the data from file. label represents the 3 documnets (0.0,0.1,0.2)\r\n",
        "sentenceData = spark.createDataFrame([\r\n",
        "        (0.0, \"Welcome to KDM TF_IDF Tutorial.\"),\r\n",
        "        (0.1, \"Learn Spark ml tf_idf in today's lab.\"),\r\n",
        "        (0.2, \"Spark Mllib has TF-IDF.\")\r\n",
        "    ], [\"label\", \"sentence\"])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwq3PckHjOwA"
      },
      "source": [
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(sentenceData)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SpLlNedjRvd",
        "outputId": "4553f93f-8378-44d6-d200-7f8f671014cc"
      },
      "source": [
        "wordsData.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Welcome to KDM TF...|[welcome, to, kdm...|\n",
            "|  0.1|Learn Spark ml tf...|[learn, spark, ml...|\n",
            "|  0.2|Spark Mllib has T...|[spark, mllib, ha...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIzuJyUSjVT2"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\r\n",
        "featurizedData = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcpFOy67jYaK"
      },
      "source": [
        "# calculating the IDF\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idfModel = idf.fit(featurizedData)\r\n",
        "rescaledData = idfModel.transform(featurizedData)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_hiq9a2jcA0",
        "outputId": "5650a3a8-51ce-45ea-e9aa-a090421e6625"
      },
      "source": [
        "#displaying the results\r\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[2,8,13,15,17...|\n",
            "|  0.1|(20,[2,3,6,7],[0....|\n",
            "|  0.2|(20,[6,14,15],[0....|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI1jQggEjfPE"
      },
      "source": [
        "spark2 = SparkSession.builder.appName(\"Ngram Example\").getOrCreate()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj1KLiGOjjOQ"
      },
      "source": [
        "#creating dataframe of input\r\n",
        "wordDataFrame = spark2.createDataFrame([\r\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\r\n",
        "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\r\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\r\n",
        "], [\"id\", \"words\"])\r\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7eFKkYLjmFv"
      },
      "source": [
        "#creating NGrams with n=2 (two words)\r\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\r\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WxE-aVCjp8d",
        "outputId": "587de2f6-9bf8-4ab2-d942-662abed31c2f"
      },
      "source": [
        "# displaying the results\r\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|ngrams                                                            |\n",
            "+------------------------------------------------------------------+\n",
            "|[Hi I, I heard, heard about, about Spark]                         |\n",
            "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
            "|[Logistic regression, regression models, models are, are neat]    |\n",
            "+------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl2J0b-TjtjD"
      },
      "source": [
        "# creating spark session\r\n",
        "spark3 = SparkSession.builder.appName(\"Word2Vec Example\").getOrCreate()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2aQaZYij24G"
      },
      "source": [
        "# Input data: Each row is a bag of words from a sentence or document.\r\n",
        "documentDF = spark3.createDataFrame([\r\n",
        "    (\"McCarthy was asked to analyse the data from the first phase of trials of the vaccine.\".split(\" \"), ),\r\n",
        "    (\"We have amassed the raw data and are about to begin analysing it.\".split(\" \"), ),\r\n",
        "    (\"Without more data we cannot make a meaningful comparison of the two systems.\".split(\" \"), ),\r\n",
        "    (\"Collecting data is a painfully slow process.\".split(\" \"), ),\r\n",
        "    (\"You need a long series of data to be able to discern such a trend.\".split(\" \"), )\r\n",
        "], [\"text\"])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqqj0Koej6mL"
      },
      "source": [
        "# Learn a mapping from words to Vectors.\r\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\r\n",
        "model = word2Vec.fit(documentDF)\r\n",
        "result = model.transform(documentDF)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fta-f5Exj-Y2",
        "outputId": "6ac1b66c-06d3-4c04-dd64-6a6375295d0d"
      },
      "source": [
        "for row in result.collect():\r\n",
        "    text, vector = row\r\n",
        "    #printing the results\r\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: [McCarthy, was, asked, to, analyse, the, data, from, the, first, phase, of, trials, of, the, vaccine.] => \n",
            "Vector: [0.022891053871717304,0.002649051835760474,0.018310461320652394]\n",
            "\n",
            "Text: [We, have, amassed, the, raw, data, and, are, about, to, begin, analysing, it.] => \n",
            "Vector: [-0.026372590818657327,-0.01868619819959769,0.04482671529359113]\n",
            "\n",
            "Text: [Without, more, data, we, cannot, make, a, meaningful, comparison, of, the, two, systems.] => \n",
            "Vector: [-0.01645793221317805,-0.03237590100616217,0.01814235510895602]\n",
            "\n",
            "Text: [Collecting, data, is, a, painfully, slow, process.] => \n",
            "Vector: [0.011914271329130444,0.027699152406837256,-0.00393283592503784]\n",
            "\n",
            "Text: [You, need, a, long, series, of, data, to, be, able, to, discern, such, a, trend.] => \n",
            "Vector: [0.05923219447334607,0.009327551970879236,0.0451309525136215]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NettMtzOkCU_",
        "outputId": "e1a4cf16-6a70-4b19-bee7-db3da8a394aa"
      },
      "source": [
        "# showing the synonyms and cosine similarity of the word in input data\r\n",
        "synonyms = model.findSynonyms(\"data\", 5)   # its okay for certain words , real bad for others\r\n",
        "synonyms.show(5)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+------------------+\n",
            "|    word|        similarity|\n",
            "+--------+------------------+\n",
            "|McCarthy|0.9495655298233032|\n",
            "|   asked|0.8464929461479187|\n",
            "|vaccine.|0.8331092596054077|\n",
            "|  trend.|0.7884795069694519|\n",
            "| analyse| 0.706946611404419|\n",
            "+--------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7nYCUQYkGod"
      },
      "source": [
        "#closing the spark sessions\r\n",
        "spark.stop()\r\n",
        "spark2.stop()\r\n",
        "spark3.stop()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35pv6mjXkJ9V"
      },
      "source": [
        "Creating 5 separate text files containing text data (blogs,news articles etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0QQWrB0kLU-"
      },
      "source": [
        "with open(\"/content/text1.txt\",\"r+\") as t1:\r\n",
        "    doc1 = t1.read()\r\n",
        "with open(\"/content/text2.txt\",\"r+\") as t2:\r\n",
        "    doc2 = t2.read()\r\n",
        "with open(\"/content/text3.txt\",\"r+\") as t3:\r\n",
        "    doc3 = t3.read()\r\n",
        "with open(\"/content/text4.txt\",\"r+\") as t4:\r\n",
        "    doc4 = t4.read()\r\n",
        "with open(\"/content/text5.txt\",\"r+\") as t5:\r\n",
        "    doc5 = t5.read()\r\n",
        "# Read all 5 txt files which contains news articles\r\n",
        "documents = [doc1,doc2,doc3,doc4,doc5]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZGwAPQ9lVa5"
      },
      "source": [
        "a.Find out the top10 TF-IDF words for the above input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF5VmyVAlgtv",
        "outputId": "7ffa826b-f279-4044-c43c-5429e83b5db4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "pd.set_option('display.max_columns', 20)\r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             to    python        is   science       the        or  knowledge  \\\n",
            "0      0.000000  0.412456  0.098269  0.000000  0.000000  0.000000   0.000000   \n",
            "1      0.099483  0.000000  0.070783  0.239692  0.239692  0.000000   0.000000   \n",
            "2      0.197250  0.000000  0.140345  0.237625  0.000000  0.000000   0.118812   \n",
            "3      0.108909  0.000000  0.077490  0.000000  0.262403  0.000000   0.524806   \n",
            "4      0.000000  0.000000  0.060893  0.000000  0.000000  0.511163   0.000000   \n",
            "Total  0.405642  0.412456  0.447780  0.477317  0.502095  0.511163   0.643618   \n",
            "\n",
            "             of      data       and  \n",
            "0      0.116185  0.000000  0.116185  \n",
            "1      0.334754  0.000000  0.251065  \n",
            "2      0.000000  0.594062  0.331866  \n",
            "3      0.091618  0.000000  0.274853  \n",
            "4      0.215985  0.206201  0.000000  \n",
            "Total  0.758542  0.800263  0.973970  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt-IvQgxlphV"
      },
      "source": [
        "b.Find out the top10 TF-IDF words for the lemmatized input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBfG1ZIPlt8p",
        "outputId": "f2c73b35-185f-42a7-94b6-c9c22d66717e"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "words1 = nltk.word_tokenize(doc1)\r\n",
        "words2 = nltk.word_tokenize(doc2)\r\n",
        "words3 = nltk.word_tokenize(doc3)\r\n",
        "words4 = nltk.word_tokenize(doc4)\r\n",
        "words5 = nltk.word_tokenize(doc5)\r\n",
        "\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in words1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in words2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in words3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in words4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in words5])\r\n",
        "\r\n",
        "documents = [lemmatized_document1,lemmatized_document2,lemmatized_document3,lemmatized_document4,lemmatized_document5]\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "         python   machine        is   science        or       the  knowledge  \\\n",
            "0      0.412456  0.000000  0.098269  0.000000  0.000000  0.000000   0.000000   \n",
            "1      0.000000  0.097930  0.069678  0.235951  0.000000  0.235951   0.000000   \n",
            "2      0.000000  0.098842  0.140654  0.238148  0.000000  0.000000   0.119074   \n",
            "3      0.000000  0.220188  0.078333  0.000000  0.000000  0.265258   0.530516   \n",
            "4      0.000000  0.000000  0.058997  0.000000  0.495243  0.000000   0.000000   \n",
            "Total  0.412456  0.416961  0.445931  0.474099  0.495243  0.501209   0.649590   \n",
            "\n",
            "             of     data       and  \n",
            "0      0.116185  0.00000  0.116185  \n",
            "1      0.329529  0.00000  0.247146  \n",
            "2      0.000000  0.59537  0.332597  \n",
            "3      0.092615  0.00000  0.277844  \n",
            "4      0.209259  0.19978  0.000000  \n",
            "Total  0.747587  0.79515  0.973772  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t78ScDal07j"
      },
      "source": [
        "c.Find out the top10TF-IDF words for the n-gram based input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klTEo949l5Ya",
        "outputId": "831c9e55-5f34-4c04-f92b-4556ad889b81"
      },
      "source": [
        "# this function takes document and n int value to generate list of n grams\r\n",
        "def ngrams(input, n):\r\n",
        "    input = input.split(' ')\r\n",
        "    output = []\r\n",
        "    for i in range(len(input)-n+1):\r\n",
        "        output.append(input[i:i+n])\r\n",
        "    return output\r\n",
        "\r\n",
        "ngram_doc1 = ' '.join([' '.join(x) for x in ngrams(doc1, 3)])\r\n",
        "ngram_doc2 = ' '.join([' '.join(x) for x in ngrams(doc2, 3)])\r\n",
        "ngram_doc3 = ' '.join([' '.join(x) for x in ngrams(doc3, 3)])\r\n",
        "ngram_doc4 = ' '.join([' '.join(x) for x in ngrams(doc4, 3)])\r\n",
        "ngram_doc5 = ' '.join([' '.join(x) for x in ngrams(doc5, 3)])\r\n",
        "\r\n",
        "# documents = [ngram_doc1,ngram_doc2,ngram_doc3,ngram_doc4,ngram_doc5]\r\n",
        "\r\n",
        "documents = [doc1,doc2,doc3,doc4,doc5]\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer( ngram_range=(3,3)) # TfidfVectorizer has inbuilt ngram kwarg which show tfidf for ngrams\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       level and general  notable use of  of significant indentation  \\\n",
            "0               0.213201        0.213201                    0.213201   \n",
            "1               0.000000        0.000000                    0.000000   \n",
            "2               0.000000        0.000000                    0.000000   \n",
            "3               0.000000        0.000000                    0.000000   \n",
            "4               0.000000        0.000000                    0.000000   \n",
            "Total           0.213201        0.213201                    0.213201   \n",
            "\n",
            "       philosophy emphasizes code  programming language python  \\\n",
            "0                        0.213201                     0.213201   \n",
            "1                        0.000000                     0.000000   \n",
            "2                        0.000000                     0.000000   \n",
            "3                        0.000000                     0.000000   \n",
            "4                        0.000000                     0.000000   \n",
            "Total                    0.213201                     0.213201   \n",
            "\n",
            "       purpose programming language  python design philosophy  \\\n",
            "0                          0.213201                  0.213201   \n",
            "1                          0.000000                  0.000000   \n",
            "2                          0.000000                  0.000000   \n",
            "3                          0.000000                  0.000000   \n",
            "4                          0.000000                  0.000000   \n",
            "Total                      0.213201                  0.213201   \n",
            "\n",
            "       general purpose programming  with its notable  data science is  \n",
            "0                         0.213201          0.213201         0.000000  \n",
            "1                         0.000000          0.000000         0.000000  \n",
            "2                         0.000000          0.000000         0.324443  \n",
            "3                         0.000000          0.000000         0.000000  \n",
            "4                         0.000000          0.000000         0.000000  \n",
            "Total                     0.213201          0.213201         0.324443  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk-Q7aLBl-rR"
      },
      "source": [
        "2.Write a simple spark program to read a dataset and find the W2V similar words (words with higher cosine similarity) for the Top10 TF-IDF Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdhA4anzmCxZ",
        "outputId": "b8e2d52e-c0cb-418d-f91d-912330444bcf"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec\r\n",
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, doc1),\r\n",
        "        (0.1, doc2),\r\n",
        "        (0.2, doc3),\r\n",
        "        (0.3, doc4),\r\n",
        "        (0.5, doc5)\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Python is an inte...|[python, is, an, ...|\n",
            "|  0.1|Computer science ...|[computer, scienc...|\n",
            "|  0.2|Data science is a...|[data, science, i...|\n",
            "|  0.3|Knowledge extract...|[knowledge, extra...|\n",
            "|  0.5|Data are characte...|[data, are, chara...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm6mh1romFcu"
      },
      "source": [
        "a.Try without NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5MDYmMjmJPu",
        "outputId": "d6e4272f-c285-4ac0-ea36-0bf6b82cd1e5"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF without NLP:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[9,20,21,43,...|\n",
            "|  0.1|(200,[9,11,16,17,...|\n",
            "|  0.2|(200,[9,17,19,24,...|\n",
            "|  0.3|(200,[9,11,17,23,...|\n",
            "|  0.5|(200,[3,6,9,15,24...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF without NLP:\n",
            "Row(label=0.0, document=\"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant indentation.\", words=['python', 'is', 'an', 'interpreted,', 'high-level', 'and', 'general-purpose', 'programming', 'language.', \"python's\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'indentation.'], rawFeatures=SparseVector(200, {9: 1.0, 20: 1.0, 21: 1.0, 43: 1.0, 50: 1.0, 67: 1.0, 73: 1.0, 80: 1.0, 90: 1.0, 91: 1.0, 95: 1.0, 109: 1.0, 110: 1.0, 119: 1.0, 120: 1.0, 122: 1.0, 128: 1.0, 133: 1.0, 152: 1.0, 163: 1.0, 179: 1.0, 198: 1.0}), features=SparseVector(200, {9: 0.0, 20: 1.0986, 21: 1.0986, 43: 0.6931, 50: 0.6931, 67: 0.1823, 73: 0.6931, 80: 1.0986, 90: 0.6931, 91: 0.1823, 95: 0.0, 109: 1.0986, 110: 1.0986, 119: 1.0986, 120: 1.0986, 122: 1.0986, 128: 1.0986, 133: 0.6931, 152: 1.0986, 163: 0.1823, 179: 1.0986, 198: 1.0986}))\n",
            "(200,[9,20,21,43,50,67,73,80,90,91,95,109,110,119,120,122,128,133,152,163,179,198],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document='Computer science is the study of algorithmic processes, computational machines and computation itself. As a discipline, computer science spans a range of topics from theoretical studies of algorithms, computation and information to the practical issues of implementing computational systems in hardware and software.', words=['computer', 'science', 'is', 'the', 'study', 'of', 'algorithmic', 'processes,', 'computational', 'machines', 'and', 'computation', 'itself.', 'as', 'a', 'discipline,', 'computer', 'science', 'spans', 'a', 'range', 'of', 'topics', 'from', 'theoretical', 'studies', 'of', 'algorithms,', 'computation', 'and', 'information', 'to', 'the', 'practical', 'issues', 'of', 'implementing', 'computational', 'systems', 'in', 'hardware', 'and', 'software.'], rawFeatures=SparseVector(200, {9: 1.0, 11: 1.0, 16: 1.0, 17: 2.0, 24: 2.0, 28: 2.0, 35: 2.0, 54: 1.0, 62: 1.0, 63: 1.0, 67: 2.0, 86: 1.0, 88: 1.0, 89: 1.0, 91: 3.0, 93: 1.0, 95: 4.0, 101: 1.0, 113: 1.0, 121: 1.0, 129: 1.0, 148: 1.0, 149: 1.0, 151: 2.0, 161: 1.0, 163: 2.0, 165: 1.0, 168: 1.0, 185: 1.0, 190: 1.0, 197: 1.0}), features=SparseVector(200, {9: 0.0, 11: 0.6931, 16: 1.0986, 17: 0.8109, 24: 0.8109, 28: 2.1972, 35: 2.1972, 54: 0.6931, 62: 1.0986, 63: 0.1823, 67: 0.3646, 86: 0.6931, 88: 0.4055, 89: 0.4055, 91: 0.547, 93: 1.0986, 95: 0.0, 101: 1.0986, 113: 1.0986, 121: 0.4055, 129: 1.0986, 148: 0.6931, 149: 1.0986, 151: 2.1972, 161: 1.0986, 163: 0.3646, 165: 1.0986, 168: 1.0986, 185: 0.6931, 190: 0.6931, 197: 1.0986}))\n",
            "(200,[9,11,16,17,24,28,35,54,62,63,67,86,88,89,91,93,95,101,113,121,129,148,149,151,161,163,165,168,185,190,197],[1.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.2, document='Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data.', words=['data', 'science', 'is', 'an', 'inter-disciplinary', 'field', 'that', 'uses', 'scientific', 'methods,', 'processes,', 'algorithms', 'and', 'systems', 'to', 'extract', 'knowledge', 'and', 'insights', 'from', 'many', 'structural', 'and', 'unstructured', 'data.', 'data', 'science', 'is', 'related', 'to', 'data', 'mining,', 'machine', 'learning', 'and', 'big', 'data.'], rawFeatures=SparseVector(200, {9: 2.0, 17: 1.0, 19: 1.0, 24: 1.0, 40: 1.0, 48: 1.0, 63: 1.0, 64: 1.0, 72: 1.0, 73: 2.0, 74: 2.0, 88: 2.0, 89: 1.0, 90: 2.0, 91: 4.0, 95: 3.0, 111: 1.0, 116: 1.0, 121: 1.0, 123: 1.0, 126: 1.0, 153: 1.0, 157: 1.0, 160: 1.0, 163: 2.0, 188: 1.0}), features=SparseVector(200, {9: 0.0, 17: 0.4055, 19: 1.0986, 24: 0.4055, 40: 1.0986, 48: 1.0986, 63: 0.1823, 64: 0.4055, 72: 1.0986, 73: 1.3863, 74: 2.1972, 88: 0.8109, 89: 0.4055, 90: 1.3863, 91: 0.7293, 95: 0.0, 111: 1.0986, 116: 0.6931, 121: 0.4055, 123: 0.6931, 126: 0.6931, 153: 1.0986, 157: 1.0986, 160: 0.4055, 163: 0.3646, 188: 1.0986}))\n",
            "(200,[9,17,19,24,40,48,63,64,72,73,74,88,89,90,91,95,111,116,121,123,126,153,157,160,163,188],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,2.0,4.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])\n",
            "Row(label=0.3, document='Knowledge extraction is the creation of knowledge from structured and unstructured sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. ', words=['knowledge', 'extraction', 'is', 'the', 'creation', 'of', 'knowledge', 'from', 'structured', 'and', 'unstructured', 'sources.', 'the', 'resulting', 'knowledge', 'needs', 'to', 'be', 'in', 'a', 'machine-readable', 'and', 'machine-interpretable', 'format', 'and', 'must', 'represent', 'knowledge', 'in', 'a', 'manner', 'that', 'facilitates', 'inferencing.'], rawFeatures=SparseVector(200, {9: 1.0, 11: 1.0, 17: 2.0, 23: 1.0, 43: 1.0, 50: 1.0, 55: 2.0, 63: 2.0, 64: 4.0, 67: 2.0, 71: 1.0, 78: 1.0, 86: 1.0, 88: 1.0, 91: 3.0, 95: 1.0, 116: 1.0, 117: 1.0, 121: 2.0, 126: 1.0, 140: 1.0, 148: 1.0, 160: 1.0, 191: 1.0}), features=SparseVector(200, {9: 0.0, 11: 0.6931, 17: 0.8109, 23: 1.0986, 43: 0.6931, 50: 0.6931, 55: 2.1972, 63: 0.3646, 64: 1.6219, 67: 0.3646, 71: 1.0986, 78: 1.0986, 86: 0.6931, 88: 0.4055, 91: 0.547, 95: 0.0, 116: 0.6931, 117: 1.0986, 121: 0.8109, 126: 0.6931, 140: 1.0986, 148: 0.6931, 160: 0.4055, 191: 1.0986}))\n",
            "(200,[9,11,17,23,43,50,55,63,64,67,71,78,86,88,91,95,116,117,121,126,140,148,160,191],[1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,4.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document='Data are characteristics or information, usually numeric, that are collected through observation. In a more technical sense, data are a set of values of qualitative or quantitative variables about one or more persons or objects, while a datum is a single value of a single variable.', words=['data', 'are', 'characteristics', 'or', 'information,', 'usually', 'numeric,', 'that', 'are', 'collected', 'through', 'observation.', 'in', 'a', 'more', 'technical', 'sense,', 'data', 'are', 'a', 'set', 'of', 'values', 'of', 'qualitative', 'or', 'quantitative', 'variables', 'about', 'one', 'or', 'more', 'persons', 'or', 'objects,', 'while', 'a', 'datum', 'is', 'a', 'single', 'value', 'of', 'a', 'single', 'variable.'], rawFeatures=SparseVector(200, {3: 1.0, 6: 1.0, 9: 2.0, 15: 1.0, 24: 1.0, 29: 2.0, 36: 1.0, 54: 1.0, 57: 1.0, 63: 1.0, 64: 1.0, 67: 5.0, 68: 1.0, 75: 1.0, 79: 1.0, 89: 2.0, 95: 5.0, 100: 1.0, 123: 1.0, 125: 1.0, 127: 1.0, 133: 1.0, 141: 5.0, 142: 1.0, 156: 1.0, 160: 1.0, 163: 3.0, 185: 1.0, 190: 1.0}), features=SparseVector(200, {3: 1.0986, 6: 1.0986, 9: 0.0, 15: 1.0986, 24: 0.4055, 29: 2.1972, 36: 1.0986, 54: 0.6931, 57: 1.0986, 63: 0.1823, 64: 0.4055, 67: 0.9116, 68: 1.0986, 75: 1.0986, 79: 1.0986, 89: 0.8109, 95: 0.0, 100: 1.0986, 123: 0.6931, 125: 1.0986, 127: 1.0986, 133: 0.6931, 141: 5.4931, 142: 1.0986, 156: 1.0986, 160: 0.4055, 163: 0.547, 185: 0.6931, 190: 0.6931}))\n",
            "(200,[3,6,9,15,24,29,36,54,57,63,64,67,68,75,79,89,95,100,123,125,127,133,141,142,156,160,163,185,190],[1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,2.0,5.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,3.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmyBi5JfmPK6"
      },
      "source": [
        "\r\n",
        "b.Try with Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OLzGi1hmUBv",
        "outputId": "cf0d638d-760e-4832-b1cd-1467da27ee79"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "words1 = nltk.word_tokenize(doc1)\r\n",
        "words2 = nltk.word_tokenize(doc2)\r\n",
        "words3 = nltk.word_tokenize(doc3)\r\n",
        "words4 = nltk.word_tokenize(doc4)\r\n",
        "words5 = nltk.word_tokenize(doc5)\r\n",
        "\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in words1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in words2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in words3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in words4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in words5])\r\n",
        "\r\n",
        "### lemmatizing words from 5 input docs same as previos task\r\n",
        "\r\n",
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, lemmatized_document1),\r\n",
        "        (0.1, lemmatized_document2),\r\n",
        "        (0.2, lemmatized_document3),\r\n",
        "        (0.3, lemmatized_document4),\r\n",
        "        (0.5, lemmatized_document5)\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Python is an inte...|[python, is, an, ...|\n",
            "|  0.1|Computer science ...|[computer, scienc...|\n",
            "|  0.2|Data science is a...|[data, science, i...|\n",
            "|  0.3|Knowledge extract...|[knowledge, extra...|\n",
            "|  0.5|Data are characte...|[data, are, chara...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YEnAtp9mYux",
        "outputId": "87e9d3d6-82b1-494a-f17f-d32e3450bdc2"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF with Lemmatization:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[9,20,21,28,...|\n",
            "|  0.1|(200,[9,16,17,24,...|\n",
            "|  0.2|(200,[9,15,17,28,...|\n",
            "|  0.3|(200,[9,11,17,23,...|\n",
            "|  0.5|(200,[3,9,11,15,2...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF with Lemmatization:\n",
            "Row(label=0.0, document=\"Python is an interpreted , high-level and general-purpose programming language . Python 's design philosophy emphasizes code readability with it notable use of significant indentation .\", words=['python', 'is', 'an', 'interpreted', ',', 'high-level', 'and', 'general-purpose', 'programming', 'language', '.', 'python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'it', 'notable', 'use', 'of', 'significant', 'indentation', '.'], rawFeatures=SparseVector(200, {9: 1.0, 20: 1.0, 21: 1.0, 28: 2.0, 50: 1.0, 63: 1.0, 67: 1.0, 73: 1.0, 86: 1.0, 90: 1.0, 91: 1.0, 95: 1.0, 109: 2.0, 110: 1.0, 116: 1.0, 119: 1.0, 120: 1.0, 122: 1.0, 133: 1.0, 152: 1.0, 163: 1.0, 179: 1.0, 188: 1.0, 197: 1.0}), features=SparseVector(200, {9: 0.0, 20: 1.0986, 21: 1.0986, 28: 0.0, 50: 0.6931, 63: 0.0, 67: 0.0, 73: 1.0986, 86: 0.4055, 90: 0.6931, 91: 0.1823, 95: 0.0, 109: 2.1972, 110: 1.0986, 116: 0.4055, 119: 1.0986, 120: 1.0986, 122: 1.0986, 133: 1.0986, 152: 1.0986, 163: 0.1823, 179: 1.0986, 188: 0.6931, 197: 1.0986}))\n",
            "(200,[9,20,21,28,50,63,67,73,86,90,91,95,109,110,116,119,120,122,133,152,163,179,188,197],[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document='Computer science is the study of algorithmic process , computational machine and computation itself . As a discipline , computer science span a range of topic from theoretical study of algorithm , computation and information to the practical issue of implementing computational system in hardware and software .', words=['computer', 'science', 'is', 'the', 'study', 'of', 'algorithmic', 'process', ',', 'computational', 'machine', 'and', 'computation', 'itself', '.', 'as', 'a', 'discipline', ',', 'computer', 'science', 'span', 'a', 'range', 'of', 'topic', 'from', 'theoretical', 'study', 'of', 'algorithm', ',', 'computation', 'and', 'information', 'to', 'the', 'practical', 'issue', 'of', 'implementing', 'computational', 'system', 'in', 'hardware', 'and', 'software', '.'], rawFeatures=SparseVector(200, {9: 1.0, 16: 2.0, 17: 2.0, 24: 2.0, 28: 4.0, 35: 2.0, 40: 2.0, 45: 1.0, 63: 2.0, 66: 1.0, 67: 5.0, 86: 1.0, 88: 1.0, 91: 3.0, 95: 4.0, 101: 1.0, 121: 1.0, 126: 1.0, 148: 1.0, 149: 1.0, 151: 2.0, 161: 1.0, 162: 1.0, 163: 2.0, 165: 1.0, 190: 1.0, 192: 1.0, 196: 1.0}), features=SparseVector(200, {9: 0.0, 16: 2.1972, 17: 0.8109, 24: 1.3863, 28: 0.0, 35: 2.1972, 40: 1.3863, 45: 1.0986, 63: 0.0, 66: 1.0986, 67: 0.0, 86: 0.4055, 88: 0.4055, 91: 0.547, 95: 0.0, 101: 1.0986, 121: 0.4055, 126: 0.4055, 148: 1.0986, 149: 1.0986, 151: 1.3863, 161: 1.0986, 162: 1.0986, 163: 0.3646, 165: 1.0986, 190: 0.6931, 192: 1.0986, 196: 0.6931}))\n",
            "(200,[9,16,17,24,28,35,40,45,63,66,67,86,88,91,95,101,121,126,148,149,151,161,162,163,165,190,192,196],[1.0,2.0,2.0,2.0,4.0,2.0,2.0,1.0,2.0,1.0,5.0,1.0,1.0,3.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.2, document='Data science is an inter-disciplinary field that us scientific method , process , algorithm and system to extract knowledge and insight from many structural and unstructured data . Data science is related to data mining , machine learning and big data .', words=['data', 'science', 'is', 'an', 'inter-disciplinary', 'field', 'that', 'us', 'scientific', 'method', ',', 'process', ',', 'algorithm', 'and', 'system', 'to', 'extract', 'knowledge', 'and', 'insight', 'from', 'many', 'structural', 'and', 'unstructured', 'data', '.', 'data', 'science', 'is', 'related', 'to', 'data', 'mining', ',', 'machine', 'learning', 'and', 'big', 'data', '.'], rawFeatures=SparseVector(200, {9: 2.0, 15: 1.0, 17: 1.0, 28: 2.0, 40: 2.0, 48: 1.0, 60: 1.0, 63: 1.0, 64: 1.0, 67: 3.0, 72: 1.0, 74: 1.0, 88: 2.0, 90: 1.0, 91: 4.0, 95: 5.0, 116: 1.0, 121: 1.0, 123: 1.0, 126: 1.0, 130: 1.0, 153: 1.0, 157: 1.0, 160: 1.0, 163: 2.0, 186: 1.0, 188: 1.0, 196: 1.0}), features=SparseVector(200, {9: 0.0, 15: 0.6931, 17: 0.4055, 28: 0.0, 40: 1.3863, 48: 1.0986, 60: 1.0986, 63: 0.0, 64: 0.4055, 67: 0.0, 72: 1.0986, 74: 1.0986, 88: 0.8109, 90: 0.6931, 91: 0.7293, 95: 0.0, 116: 0.4055, 121: 0.4055, 123: 1.0986, 126: 0.4055, 130: 1.0986, 153: 1.0986, 157: 1.0986, 160: 0.4055, 163: 0.3646, 186: 1.0986, 188: 0.6931, 196: 0.6931}))\n",
            "(200,[9,15,17,28,40,48,60,63,64,67,72,74,88,90,91,95,116,121,123,126,130,153,157,160,163,186,188,196],[2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,4.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document='Knowledge extraction is the creation of knowledge from structured and unstructured source . The resulting knowledge need to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing .', words=['knowledge', 'extraction', 'is', 'the', 'creation', 'of', 'knowledge', 'from', 'structured', 'and', 'unstructured', 'source', '.', 'the', 'resulting', 'knowledge', 'need', 'to', 'be', 'in', 'a', 'machine-readable', 'and', 'machine-interpretable', 'format', 'and', 'must', 'represent', 'knowledge', 'in', 'a', 'manner', 'that', 'facilitates', 'inferencing', '.'], rawFeatures=SparseVector(200, {9: 1.0, 11: 1.0, 17: 2.0, 23: 1.0, 28: 2.0, 43: 1.0, 50: 1.0, 55: 2.0, 63: 2.0, 64: 4.0, 67: 2.0, 71: 1.0, 86: 1.0, 88: 1.0, 91: 3.0, 95: 1.0, 116: 1.0, 117: 1.0, 121: 3.0, 126: 1.0, 137: 1.0, 140: 1.0, 151: 1.0, 160: 1.0}), features=SparseVector(200, {9: 0.0, 11: 0.6931, 17: 0.8109, 23: 1.0986, 28: 0.0, 43: 1.0986, 50: 0.6931, 55: 2.1972, 63: 0.0, 64: 1.6219, 67: 0.0, 71: 1.0986, 86: 0.4055, 88: 0.4055, 91: 0.547, 95: 0.0, 116: 0.4055, 117: 1.0986, 121: 1.2164, 126: 0.4055, 137: 1.0986, 140: 1.0986, 151: 0.6931, 160: 0.4055}))\n",
            "(200,[9,11,17,23,28,43,50,55,63,64,67,71,86,88,91,95,116,117,121,126,137,140,151,160],[1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,4.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document='Data are characteristic or information , usually numeric , that are collected through observation . In a more technical sense , data are a set of value of qualitative or quantitative variable about one or more person or object , while a datum is a single value of a single variable .', words=['data', 'are', 'characteristic', 'or', 'information', ',', 'usually', 'numeric', ',', 'that', 'are', 'collected', 'through', 'observation', '.', 'in', 'a', 'more', 'technical', 'sense', ',', 'data', 'are', 'a', 'set', 'of', 'value', 'of', 'qualitative', 'or', 'quantitative', 'variable', 'about', 'one', 'or', 'more', 'person', 'or', 'object', ',', 'while', 'a', 'datum', 'is', 'a', 'single', 'value', 'of', 'a', 'single', 'variable', '.'], rawFeatures=SparseVector(200, {3: 2.0, 9: 1.0, 11: 1.0, 15: 1.0, 24: 1.0, 27: 2.0, 28: 2.0, 29: 2.0, 47: 1.0, 49: 1.0, 53: 1.0, 61: 1.0, 63: 1.0, 64: 1.0, 67: 9.0, 68: 1.0, 75: 1.0, 89: 2.0, 95: 5.0, 125: 2.0, 141: 5.0, 142: 1.0, 156: 1.0, 160: 1.0, 163: 3.0, 185: 1.0, 190: 2.0}), features=SparseVector(200, {3: 2.1972, 9: 0.0, 11: 0.6931, 15: 0.6931, 24: 0.6931, 27: 2.1972, 28: 0.0, 29: 2.1972, 47: 1.0986, 49: 1.0986, 53: 1.0986, 61: 1.0986, 63: 0.0, 64: 0.4055, 67: 0.0, 68: 1.0986, 75: 1.0986, 89: 2.1972, 95: 0.0, 125: 2.1972, 141: 5.4931, 142: 1.0986, 156: 1.0986, 160: 0.4055, 163: 0.547, 185: 1.0986, 190: 1.3863}))\n",
            "(200,[3,9,11,15,24,27,28,29,47,49,53,61,63,64,67,68,75,89,95,125,141,142,156,160,163,185,190],[2.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,2.0,5.0,2.0,5.0,1.0,1.0,1.0,3.0,1.0,2.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgrDWH1Dmbs-"
      },
      "source": [
        "c.Try with NGrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEr0kChOmgH0",
        "outputId": "5250e3f3-a595-4b60-8cc9-9deeae56ff17"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, doc1.split(' ')),\r\n",
        "        (0.1, doc2.split(' ')),\r\n",
        "        (0.2, doc3.split(' ')),\r\n",
        "        (0.3, doc4.split(' ')),\r\n",
        "        (0.5, doc5.split(' '))\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "\r\n",
        "ngram = NGram(n=2, inputCol=\"document\", outputCol=\"ngrams\")\r\n",
        "\r\n",
        "ngramDataFrame = ngram.transform(documentData)\r\n",
        "\r\n",
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(ngramDataFrame)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF with ngram:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[1,9,15,20,2...|\n",
            "|  0.1|(200,[4,7,9,11,14...|\n",
            "|  0.2|(200,[13,14,18,19...|\n",
            "|  0.3|(200,[1,10,15,16,...|\n",
            "|  0.5|(200,[15,22,25,31...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF with ngram:\n",
            "Row(label=0.0, document=['Python', 'is', 'an', 'interpreted,', 'high-level', 'and', 'general-purpose', 'programming', 'language.', \"Python's\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'indentation.'], ngrams=['Python is', 'is an', 'an interpreted,', 'interpreted, high-level', 'high-level and', 'and general-purpose', 'general-purpose programming', 'programming language.', \"language. Python's\", \"Python's design\", 'design philosophy', 'philosophy emphasizes', 'emphasizes code', 'code readability', 'readability with', 'with its', 'its notable', 'notable use', 'use of', 'of significant', 'significant indentation.'], rawFeatures=SparseVector(200, {1: 2.0, 9: 1.0, 15: 1.0, 20: 1.0, 22: 1.0, 29: 1.0, 46: 1.0, 56: 1.0, 57: 1.0, 73: 1.0, 115: 1.0, 133: 1.0, 136: 1.0, 154: 1.0, 161: 1.0, 181: 2.0, 184: 2.0, 195: 1.0}), features=SparseVector(200, {1: 1.3863, 9: 0.6931, 15: 0.4055, 20: 1.0986, 22: 0.6931, 29: 1.0986, 46: 1.0986, 56: 0.4055, 57: 0.6931, 73: 1.0986, 115: 1.0986, 133: 0.6931, 136: 0.6931, 154: 1.0986, 161: 0.4055, 181: 2.1972, 184: 2.1972, 195: 0.6931}))\n",
            "(200,[1,9,15,20,22,29,46,56,57,73,115,133,136,154,161,181,184,195],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0])\n",
            "Row(label=0.1, document=['Computer', 'science', 'is', 'the', 'study', 'of', 'algorithmic', 'processes,', 'computational', 'machines', 'and', 'computation', 'itself.', 'As', 'a', 'discipline,', 'computer', 'science', 'spans', 'a', 'range', 'of', 'topics', 'from', 'theoretical', 'studies', 'of', 'algorithms,', 'computation', 'and', 'information', 'to', 'the', 'practical', 'issues', 'of', 'implementing', 'computational', 'systems', 'in', 'hardware', 'and', 'software.'], ngrams=['Computer science', 'science is', 'is the', 'the study', 'study of', 'of algorithmic', 'algorithmic processes,', 'processes, computational', 'computational machines', 'machines and', 'and computation', 'computation itself.', 'itself. As', 'As a', 'a discipline,', 'discipline, computer', 'computer science', 'science spans', 'spans a', 'a range', 'range of', 'of topics', 'topics from', 'from theoretical', 'theoretical studies', 'studies of', 'of algorithms,', 'algorithms, computation', 'computation and', 'and information', 'information to', 'to the', 'the practical', 'practical issues', 'issues of', 'of implementing', 'implementing computational', 'computational systems', 'systems in', 'in hardware', 'hardware and', 'and software.'], rawFeatures=SparseVector(200, {4: 2.0, 7: 2.0, 9: 1.0, 11: 1.0, 14: 1.0, 19: 1.0, 27: 1.0, 51: 1.0, 59: 1.0, 60: 1.0, 66: 1.0, 71: 2.0, 85: 1.0, 89: 1.0, 92: 1.0, 93: 1.0, 94: 2.0, 96: 1.0, 102: 1.0, 107: 2.0, 108: 1.0, 109: 1.0, 110: 1.0, 126: 2.0, 130: 1.0, 140: 1.0, 145: 1.0, 163: 1.0, 168: 1.0, 171: 2.0, 172: 2.0, 174: 1.0, 178: 1.0, 186: 1.0}), features=SparseVector(200, {4: 2.1972, 7: 2.1972, 9: 0.6931, 11: 1.0986, 14: 0.6931, 19: 0.6931, 27: 1.0986, 51: 0.6931, 59: 1.0986, 60: 0.6931, 66: 1.0986, 71: 2.1972, 85: 0.6931, 89: 1.0986, 92: 1.0986, 93: 1.0986, 94: 1.3863, 96: 1.0986, 102: 0.6931, 107: 2.1972, 108: 0.6931, 109: 1.0986, 110: 1.0986, 126: 1.3863, 130: 0.6931, 140: 1.0986, 145: 0.4055, 163: 0.4055, 168: 0.4055, 171: 2.1972, 172: 2.1972, 174: 1.0986, 178: 1.0986, 186: 0.4055}))\n",
            "(200,[4,7,9,11,14,19,27,51,59,60,66,71,85,89,92,93,94,96,102,107,108,109,110,126,130,140,145,163,168,171,172,174,178,186],[2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0])\n",
            "Row(label=0.2, document=['Data', 'science', 'is', 'an', 'inter-disciplinary', 'field', 'that', 'uses', 'scientific', 'methods,', 'processes,', 'algorithms', 'and', 'systems', 'to', 'extract', 'knowledge', 'and', 'insights', 'from', 'many', 'structural', 'and', 'unstructured', 'data.', 'Data', 'science', 'is', 'related', 'to', 'data', 'mining,', 'machine', 'learning', 'and', 'big', 'data.'], ngrams=['Data science', 'science is', 'is an', 'an inter-disciplinary', 'inter-disciplinary field', 'field that', 'that uses', 'uses scientific', 'scientific methods,', 'methods, processes,', 'processes, algorithms', 'algorithms and', 'and systems', 'systems to', 'to extract', 'extract knowledge', 'knowledge and', 'and insights', 'insights from', 'from many', 'many structural', 'structural and', 'and unstructured', 'unstructured data.', 'data. Data', 'Data science', 'science is', 'is related', 'related to', 'to data', 'data mining,', 'mining, machine', 'machine learning', 'learning and', 'and big', 'big data.'], rawFeatures=SparseVector(200, {13: 1.0, 14: 1.0, 18: 2.0, 19: 1.0, 30: 1.0, 34: 1.0, 50: 1.0, 56: 1.0, 64: 1.0, 67: 1.0, 70: 1.0, 72: 1.0, 75: 1.0, 77: 1.0, 85: 1.0, 87: 1.0, 90: 2.0, 94: 1.0, 95: 1.0, 97: 1.0, 98: 1.0, 108: 1.0, 119: 1.0, 127: 1.0, 133: 1.0, 145: 2.0, 146: 1.0, 157: 1.0, 163: 1.0, 168: 1.0, 173: 1.0, 186: 1.0, 192: 1.0}), features=SparseVector(200, {13: 1.0986, 14: 0.6931, 18: 2.1972, 19: 0.6931, 30: 0.6931, 34: 1.0986, 50: 1.0986, 56: 0.4055, 64: 1.0986, 67: 1.0986, 70: 0.6931, 72: 0.4055, 75: 1.0986, 77: 1.0986, 85: 0.6931, 87: 1.0986, 90: 1.3863, 94: 0.6931, 95: 0.6931, 97: 1.0986, 98: 1.0986, 108: 0.6931, 119: 0.4055, 127: 1.0986, 133: 0.6931, 145: 0.8109, 146: 0.6931, 157: 1.0986, 163: 0.4055, 168: 0.4055, 173: 1.0986, 186: 0.4055, 192: 1.0986}))\n",
            "(200,[13,14,18,19,30,34,50,56,64,67,70,72,75,77,85,87,90,94,95,97,98,108,119,127,133,145,146,157,163,168,173,186,192],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document=['Knowledge', 'extraction', 'is', 'the', 'creation', 'of', 'knowledge', 'from', 'structured', 'and', 'unstructured', 'sources.', 'The', 'resulting', 'knowledge', 'needs', 'to', 'be', 'in', 'a', 'machine-readable', 'and', 'machine-interpretable', 'format', 'and', 'must', 'represent', 'knowledge', 'in', 'a', 'manner', 'that', 'facilitates', 'inferencing.', ''], ngrams=['Knowledge extraction', 'extraction is', 'is the', 'the creation', 'creation of', 'of knowledge', 'knowledge from', 'from structured', 'structured and', 'and unstructured', 'unstructured sources.', 'sources. The', 'The resulting', 'resulting knowledge', 'knowledge needs', 'needs to', 'to be', 'be in', 'in a', 'a machine-readable', 'machine-readable and', 'and machine-interpretable', 'machine-interpretable format', 'format and', 'and must', 'must represent', 'represent knowledge', 'knowledge in', 'in a', 'a manner', 'manner that', 'that facilitates', 'facilitates inferencing.', 'inferencing. '], rawFeatures=SparseVector(200, {1: 1.0, 10: 1.0, 15: 1.0, 16: 1.0, 30: 2.0, 44: 1.0, 49: 1.0, 51: 2.0, 70: 1.0, 72: 1.0, 78: 1.0, 81: 1.0, 90: 1.0, 100: 1.0, 102: 1.0, 106: 1.0, 114: 1.0, 119: 1.0, 120: 1.0, 130: 1.0, 131: 1.0, 136: 1.0, 142: 1.0, 146: 1.0, 150: 1.0, 151: 1.0, 152: 1.0, 161: 1.0, 182: 1.0, 187: 1.0, 194: 1.0, 196: 1.0}), features=SparseVector(200, {1: 0.6931, 10: 1.0986, 15: 0.4055, 16: 1.0986, 30: 1.3863, 44: 1.0986, 49: 1.0986, 51: 1.3863, 70: 0.6931, 72: 0.4055, 78: 1.0986, 81: 1.0986, 90: 0.6931, 100: 1.0986, 102: 0.6931, 106: 1.0986, 114: 1.0986, 119: 0.4055, 120: 1.0986, 130: 0.6931, 131: 0.6931, 136: 0.6931, 142: 0.6931, 146: 0.6931, 150: 0.6931, 151: 1.0986, 152: 1.0986, 161: 0.4055, 182: 0.6931, 187: 0.6931, 194: 0.6931, 196: 0.6931}))\n",
            "(200,[1,10,15,16,30,44,49,51,70,72,78,81,90,100,102,106,114,119,120,130,131,136,142,146,150,151,152,161,182,187,194,196],[1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document=['Data', 'are', 'characteristics', 'or', 'information,', 'usually', 'numeric,', 'that', 'are', 'collected', 'through', 'observation.', 'In', 'a', 'more', 'technical', 'sense,', 'data', 'are', 'a', 'set', 'of', 'values', 'of', 'qualitative', 'or', 'quantitative', 'variables', 'about', 'one', 'or', 'more', 'persons', 'or', 'objects,', 'while', 'a', 'datum', 'is', 'a', 'single', 'value', 'of', 'a', 'single', 'variable.'], ngrams=['Data are', 'are characteristics', 'characteristics or', 'or information,', 'information, usually', 'usually numeric,', 'numeric, that', 'that are', 'are collected', 'collected through', 'through observation.', 'observation. In', 'In a', 'a more', 'more technical', 'technical sense,', 'sense, data', 'data are', 'are a', 'a set', 'set of', 'of values', 'values of', 'of qualitative', 'qualitative or', 'or quantitative', 'quantitative variables', 'variables about', 'about one', 'one or', 'or more', 'more persons', 'persons or', 'or objects,', 'objects, while', 'while a', 'a datum', 'datum is', 'is a', 'a single', 'single value', 'value of', 'of a', 'a single', 'single variable.'], rawFeatures=SparseVector(200, {15: 1.0, 22: 1.0, 25: 1.0, 31: 1.0, 54: 2.0, 56: 1.0, 57: 2.0, 60: 1.0, 72: 1.0, 83: 1.0, 84: 1.0, 95: 2.0, 113: 1.0, 119: 2.0, 126: 1.0, 131: 2.0, 132: 1.0, 142: 2.0, 143: 1.0, 145: 1.0, 150: 1.0, 156: 2.0, 161: 1.0, 163: 1.0, 168: 1.0, 169: 2.0, 180: 1.0, 182: 1.0, 183: 1.0, 185: 1.0, 186: 1.0, 187: 2.0, 190: 1.0, 194: 1.0, 195: 1.0, 196: 1.0}), features=SparseVector(200, {15: 0.4055, 22: 0.6931, 25: 1.0986, 31: 1.0986, 54: 2.1972, 56: 0.4055, 57: 1.3863, 60: 0.6931, 72: 0.4055, 83: 1.0986, 84: 1.0986, 95: 1.3863, 113: 1.0986, 119: 0.8109, 126: 0.6931, 131: 1.3863, 132: 1.0986, 142: 1.3863, 143: 1.0986, 145: 0.4055, 150: 0.6931, 156: 2.1972, 161: 0.4055, 163: 0.4055, 168: 0.4055, 169: 2.1972, 180: 1.0986, 182: 0.6931, 183: 1.0986, 185: 1.0986, 186: 0.4055, 187: 1.3863, 190: 1.0986, 194: 0.6931, 195: 0.6931, 196: 0.6931}))\n",
            "(200,[15,22,25,31,54,56,57,60,72,83,84,95,113,119,126,131,132,142,143,145,150,156,161,163,168,169,180,182,183,185,186,187,190,194,195,196],[1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
